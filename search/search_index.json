{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction The Open Model Interface (OMI) proposes a spec for multi-platform OCI-compatible container images for Machine Learning models. We believe that ML teams should be able to build reusable, DevOps-ready container images for interoperable consumption by multiple platforms to avoid being locked in to any one platform. Key points Models are baked into the container image, not downloaded at runtime Model serving framework can be switched at runtime by environment variable Pre- and post-processing logic can be shipped with the container image Spec includes recommendations around model management for reproducibility & provenance First reference implementation of OMI is available, Apache 2 licensed, at Chassis.ml Model Baking Including the model files in the container image itself is better than downloading them at runtime because it makes the model images self-contained, adheres to DevOps best practice, and improves reproducibility and reliability. In particular, some systems download a model from, say, an MLflow server when the container starts up. This relies on the MLflow server being online, which makes the MLflow server a critical part of your production infrastructure. As MLflow servers are often managed by ML teams, this is undesirable, and can result in downtime for your ML services if your MLflow server is unavailable when your model containers restart or scale up. In terms of reproducibility, baking models into containers is better because your CI/CD system can record the exact version of the container that was running at any given time, and so you can trace a specific inference result to the exact model that was running in production at that moment, and then trace it (see \"Origin tags\" in the spec) back to the exact model that trained it. Build once, run many By creating images that are \"multi-purpose\", you can avoid lock-in from a specific runtime platform. Instead, your ML teams can build container images that work in a variety of different platforms and are \"portable\" between different platforms with no effort. See the spec to see exactly how runtime configuration of the exposed interface works. Reference Implementation The first reference implementation of the OMI is available today, under the Apache 2 license, at Chassis.ml . There you can try a test drive of the reference implementation to play with it today. Roadmap Support for other runtimes: Sagemaker, Algorithmia, etc Ingest models from sources other than MLflow Explainability support Drift support History The OMI is founded by Modzy is a commercial ModelOps platform designed to run any kind of machine learning and artifical intelligence model in production, at scale, with enterprise grade security, governance, and compliance. The design of the Open Model Interface grew out of Modzy's internal research and development to provide a common spec that would support all kinds of models, both present and future, with first-class support for emerging capabilities like drift detection, explainability, and adversarial defense.","title":"Introduction"},{"location":"#introduction","text":"The Open Model Interface (OMI) proposes a spec for multi-platform OCI-compatible container images for Machine Learning models. We believe that ML teams should be able to build reusable, DevOps-ready container images for interoperable consumption by multiple platforms to avoid being locked in to any one platform.","title":"Introduction"},{"location":"#key-points","text":"Models are baked into the container image, not downloaded at runtime Model serving framework can be switched at runtime by environment variable Pre- and post-processing logic can be shipped with the container image Spec includes recommendations around model management for reproducibility & provenance First reference implementation of OMI is available, Apache 2 licensed, at Chassis.ml","title":"Key points"},{"location":"#model-baking","text":"Including the model files in the container image itself is better than downloading them at runtime because it makes the model images self-contained, adheres to DevOps best practice, and improves reproducibility and reliability. In particular, some systems download a model from, say, an MLflow server when the container starts up. This relies on the MLflow server being online, which makes the MLflow server a critical part of your production infrastructure. As MLflow servers are often managed by ML teams, this is undesirable, and can result in downtime for your ML services if your MLflow server is unavailable when your model containers restart or scale up. In terms of reproducibility, baking models into containers is better because your CI/CD system can record the exact version of the container that was running at any given time, and so you can trace a specific inference result to the exact model that was running in production at that moment, and then trace it (see \"Origin tags\" in the spec) back to the exact model that trained it.","title":"Model Baking"},{"location":"#build-once-run-many","text":"By creating images that are \"multi-purpose\", you can avoid lock-in from a specific runtime platform. Instead, your ML teams can build container images that work in a variety of different platforms and are \"portable\" between different platforms with no effort. See the spec to see exactly how runtime configuration of the exposed interface works.","title":"Build once, run many"},{"location":"#reference-implementation","text":"The first reference implementation of the OMI is available today, under the Apache 2 license, at Chassis.ml . There you can try a test drive of the reference implementation to play with it today.","title":"Reference Implementation"},{"location":"#roadmap","text":"Support for other runtimes: Sagemaker, Algorithmia, etc Ingest models from sources other than MLflow Explainability support Drift support","title":"Roadmap"},{"location":"#history","text":"The OMI is founded by Modzy is a commercial ModelOps platform designed to run any kind of machine learning and artifical intelligence model in production, at scale, with enterprise grade security, governance, and compliance. The design of the Open Model Interface grew out of Modzy's internal research and development to provide a common spec that would support all kinds of models, both present and future, with first-class support for emerging capabilities like drift detection, explainability, and adversarial defense.","title":"History"},{"location":"rationale/","text":"Rationale Why Open Model Interface? As machine learning and artificial intelligence become more wide-spread and with the rapid pace of innovation and invention there can often be barriers to entry in deploying models into production. New models need to undergo security evaluation and review and often times engineering effort needs to be scheduled to integrate those models into the systems, applications, and workflows where they will bring value. Various surveys indicate that this process can lead to a 6 month or greater delay between when a model is ready for production and when it's available for use in production. The Open Model Interface is designed to serve as a spec for wrapping models in OCI-compliant containers with a simple yet powerful interface that standardizes on certain security best practices, supports the widest range of machine learning and artificial intelligence training tools and frameworks, and enables re-use of existing integration code to add new or updated models to production environments by sharing a common API across all model containers. The spec itself tries to be as agnostic as possible as to what the model is and how it was trained. This is important because the world of machine learning and artificial intelligence is evolving so rapidly that any choices or limitations made now will inevitably become blockers to adoption in just a short amount of time. So the spec endeavors to make no assumptions about the language, framework, dependencies, or architectures of the model itself, save for its ability to be packaged as an OCI-compliant container image. The interface to the model as defined by the spec is a simple gRPC service that is embedded in the container image that exposes a small handful of routes to handle communication between users of the model and the model itself. Using a common gRPC interface to get status info about the model, submit data for inference, and initiate cleanup/shutdown sequence allows for a wide diversity of models to support existing and as-yet-not-invented techniques and architectures to be supported. Finally the Open Model Interface spec includes out-of-the-box support for deployment with popular ModelOps platforms so that models that adhere to this spec can be added to such platforms for increased scale, security, governance, and compliance. Models in production are rarely deployed alone or in a vacuum and while the Open Model Interface spec provides for a common API to interact and integrate with the model, it's important that models that adhere to this spec are not only useful in isolation but can be seamlessly used by platforms and tools that provide the full suite of ModelOps capabilities. Today the spec supports seamless deployment and usage with KFServing and Modzy with room to add support for more ModelOps platforms in the future. Why gRPC? The choice to use gRPC as the transport protocol for communication between users and the model is due to the wide flexibility in communication patterns that gRPC supports. While choosing JSON and REST presents a low barrier to entry, REST doesn't support more advanced and flexible communication patterns like uni- and bi-directional streaming which are crucial for many of today's more advanced models and usage patterns. The gRPC protocol is not only seeing increased usage in the world of cloud computing and microservices but it also supports all the communication patterns that are valuable for feeding data to models to perform inference. It supports single-shot data submission, batch, stream in, stream out, and bi-directional streams. Additionally, the ability to publish a protobuf file with the typed interface and RPC specification makes for extremely intuitive implementation for software developers wanting to integrate models into their applications and workflows. Using protobufs as the data encoding also increases efficiency and performance of data transmission which can make a real difference in high-performance and low-latency scenarios and reduce time and cost for data transmission over the wire for large applications of AI/ML, especially when running in the cloud. Why KFServing? The choice to include out-of-the-box support for KFService is due to the fact that it is open source and part of the widely used KubeFlow project. KFService provides an open source solution to running many Open Model Interface-compliant models in production with some additional ModelOps capabilities. Why Modzy? Modzy is a commercial ModelOps platform designed to run any kind of machine learning and artifical intelligence model in production, at scale, with enterprise grade security, governance, and compliance. The design of the Open Model Interface grew out of Modzy's internal research and development to provide a common spec that would support all kinds of models, both present and future, with first-class support for emerging capabilities like drift detection, explainability, and adversarial defense.","title":"Rationale"},{"location":"rationale/#rationale","text":"","title":"Rationale"},{"location":"rationale/#why-open-model-interface","text":"As machine learning and artificial intelligence become more wide-spread and with the rapid pace of innovation and invention there can often be barriers to entry in deploying models into production. New models need to undergo security evaluation and review and often times engineering effort needs to be scheduled to integrate those models into the systems, applications, and workflows where they will bring value. Various surveys indicate that this process can lead to a 6 month or greater delay between when a model is ready for production and when it's available for use in production. The Open Model Interface is designed to serve as a spec for wrapping models in OCI-compliant containers with a simple yet powerful interface that standardizes on certain security best practices, supports the widest range of machine learning and artificial intelligence training tools and frameworks, and enables re-use of existing integration code to add new or updated models to production environments by sharing a common API across all model containers. The spec itself tries to be as agnostic as possible as to what the model is and how it was trained. This is important because the world of machine learning and artificial intelligence is evolving so rapidly that any choices or limitations made now will inevitably become blockers to adoption in just a short amount of time. So the spec endeavors to make no assumptions about the language, framework, dependencies, or architectures of the model itself, save for its ability to be packaged as an OCI-compliant container image. The interface to the model as defined by the spec is a simple gRPC service that is embedded in the container image that exposes a small handful of routes to handle communication between users of the model and the model itself. Using a common gRPC interface to get status info about the model, submit data for inference, and initiate cleanup/shutdown sequence allows for a wide diversity of models to support existing and as-yet-not-invented techniques and architectures to be supported. Finally the Open Model Interface spec includes out-of-the-box support for deployment with popular ModelOps platforms so that models that adhere to this spec can be added to such platforms for increased scale, security, governance, and compliance. Models in production are rarely deployed alone or in a vacuum and while the Open Model Interface spec provides for a common API to interact and integrate with the model, it's important that models that adhere to this spec are not only useful in isolation but can be seamlessly used by platforms and tools that provide the full suite of ModelOps capabilities. Today the spec supports seamless deployment and usage with KFServing and Modzy with room to add support for more ModelOps platforms in the future.","title":"Why Open Model Interface?"},{"location":"rationale/#why-grpc","text":"The choice to use gRPC as the transport protocol for communication between users and the model is due to the wide flexibility in communication patterns that gRPC supports. While choosing JSON and REST presents a low barrier to entry, REST doesn't support more advanced and flexible communication patterns like uni- and bi-directional streaming which are crucial for many of today's more advanced models and usage patterns. The gRPC protocol is not only seeing increased usage in the world of cloud computing and microservices but it also supports all the communication patterns that are valuable for feeding data to models to perform inference. It supports single-shot data submission, batch, stream in, stream out, and bi-directional streams. Additionally, the ability to publish a protobuf file with the typed interface and RPC specification makes for extremely intuitive implementation for software developers wanting to integrate models into their applications and workflows. Using protobufs as the data encoding also increases efficiency and performance of data transmission which can make a real difference in high-performance and low-latency scenarios and reduce time and cost for data transmission over the wire for large applications of AI/ML, especially when running in the cloud.","title":"Why gRPC?"},{"location":"rationale/#why-kfserving","text":"The choice to include out-of-the-box support for KFService is due to the fact that it is open source and part of the widely used KubeFlow project. KFService provides an open source solution to running many Open Model Interface-compliant models in production with some additional ModelOps capabilities.","title":"Why KFServing?"},{"location":"rationale/#why-modzy","text":"Modzy is a commercial ModelOps platform designed to run any kind of machine learning and artifical intelligence model in production, at scale, with enterprise grade security, governance, and compliance. The design of the Open Model Interface grew out of Modzy's internal research and development to provide a common spec that would support all kinds of models, both present and future, with first-class support for emerging capabilities like drift detection, explainability, and adversarial defense.","title":"Why Modzy?"},{"location":"spec/","text":"Open Model Interface Spec The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in RFC 2119 . Changelog Date Version Notes 2021-07-26 1.0 Initial version of OMI Spec Requirements To qualify as an Open Model Interface (OMI) compliant container, a container must pass the following tests. OCI compliant The container image MUST be an OCI compliant container image, for example it MAY be generated by docker build or Kaniko . Origin tags The container MUST be tagged (e.g. tag in registry/org/image:tag ) with a unique ID which corresponds to the ID of the model in the upstream model management system. For example, the container image tag MAY be set to the MLflow Run ID which generated the model. This is to enable provenance tracking from a given model which generated certain inference results to metadata about how the model was trained. The metadata referred to by the model tag SHOULD record the versions of the code, data and environment that were used to train the model. No latest tag at runtime The container MUST NOT be addressed via the OCI image tag :latest when the serving system is configured to refer to the model. This is to ensure better reproducibility and tracking of which model generated certain inference results. It is up to the model serving system (or the CI/CD system which is driving it) to keep track of which models were deployed/configured at runtime in order to be able to track back from a given model inference to exactly which version of the model generated that inference. Environment variables The container MUST be configurable with the following environment variables (the values below are just examples): env : - name : INTERFACE value : kfserving - name : HTTP_PORT value : \"8080\" - name : PROTOCOL value : v1 - name : MODEL_NAME value : digits INTERFACE MUST be the string interface supported, as defined in the interfaces section below. PROTOCOL MAY be the optional sub-protocol of the given interface MODEL_NAME MUST be some human-readable name of the model that is to be run inside the container. This CAN be used to switch between different bundled models at runtime HTTP_PORT MUST be the TCP port that the container should listen on when started Declares which interfaces it supports The container MUST have an OCI annotation of the following form: ml.openmodel.interfaces=[\"kfserving.v1\", \"modzy.v2\"] Where ml.openmodel.interfaces is a JSON formatted list of . -delimited (INTERFACE, PROTOCOL) tuples. This is so that the serving system at runtime can determine whether it supports a given model without trying to run it so that it can give the user fast feedback. For example, if a container declares that it supports the KFServing v1 API and the Modzy v2 API as per the example above, it MUST support running in both of the following configurations: env : - name : INTERFACE value : kfserving - name : PROTOCOL value : v1 env : - name : INTERFACE value : modzy - name : PROTOCOL value : v2 Ports When started, the container image MUST listen on the TCP port given as the HTTP_PORT environment variable. When a container starts listening on the prescribed port, it MUST adhere to the interface described below corresponding to its configuration. Interfaces The following interfaces are permitted in the spec: kfserving : Supports the KFServing v1 REST API with INTERFACE=kfserving, PROTOCOL=v1 and/or the v2 gRPC API with INTERFACE=kfserving, PROTOCOL=v2 modzy : Supports the Modzy v1 REST API with INTERFACE=modzy, PROTOCOL=v1 or the Modzy v2 gRPC API with INTERFACE=modzy, PROTOCOL=v2 OMI compliant container images MUST implement at least the kfserving.v2 and modzy.v2 APIs. The Chassis reference implementation implements kfserving.v1 , kfserving.v2 and modzy.v2 . Updates to this spec If an OMI image declares or supports any strings other than the ones defined above, this document MUST be updated with the new interfaces (with links to their upstream protocol specifications), otherwise the container MUST NOT be described as Open Model Interface compliant. This document can be updated via Pull Request on GitHub . Contributions welcome!","title":"Open Model Interface Spec"},{"location":"spec/#open-model-interface-spec","text":"The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\", \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in RFC 2119 .","title":"Open Model Interface Spec"},{"location":"spec/#changelog","text":"Date Version Notes 2021-07-26 1.0 Initial version of OMI Spec","title":"Changelog"},{"location":"spec/#requirements","text":"To qualify as an Open Model Interface (OMI) compliant container, a container must pass the following tests.","title":"Requirements"},{"location":"spec/#oci-compliant","text":"The container image MUST be an OCI compliant container image, for example it MAY be generated by docker build or Kaniko .","title":"OCI compliant"},{"location":"spec/#origin-tags","text":"The container MUST be tagged (e.g. tag in registry/org/image:tag ) with a unique ID which corresponds to the ID of the model in the upstream model management system. For example, the container image tag MAY be set to the MLflow Run ID which generated the model. This is to enable provenance tracking from a given model which generated certain inference results to metadata about how the model was trained. The metadata referred to by the model tag SHOULD record the versions of the code, data and environment that were used to train the model.","title":"Origin tags"},{"location":"spec/#no-latest-tag-at-runtime","text":"The container MUST NOT be addressed via the OCI image tag :latest when the serving system is configured to refer to the model. This is to ensure better reproducibility and tracking of which model generated certain inference results. It is up to the model serving system (or the CI/CD system which is driving it) to keep track of which models were deployed/configured at runtime in order to be able to track back from a given model inference to exactly which version of the model generated that inference.","title":"No latest tag at runtime"},{"location":"spec/#environment-variables","text":"The container MUST be configurable with the following environment variables (the values below are just examples): env : - name : INTERFACE value : kfserving - name : HTTP_PORT value : \"8080\" - name : PROTOCOL value : v1 - name : MODEL_NAME value : digits INTERFACE MUST be the string interface supported, as defined in the interfaces section below. PROTOCOL MAY be the optional sub-protocol of the given interface MODEL_NAME MUST be some human-readable name of the model that is to be run inside the container. This CAN be used to switch between different bundled models at runtime HTTP_PORT MUST be the TCP port that the container should listen on when started","title":"Environment variables"},{"location":"spec/#declares-which-interfaces-it-supports","text":"The container MUST have an OCI annotation of the following form: ml.openmodel.interfaces=[\"kfserving.v1\", \"modzy.v2\"] Where ml.openmodel.interfaces is a JSON formatted list of . -delimited (INTERFACE, PROTOCOL) tuples. This is so that the serving system at runtime can determine whether it supports a given model without trying to run it so that it can give the user fast feedback. For example, if a container declares that it supports the KFServing v1 API and the Modzy v2 API as per the example above, it MUST support running in both of the following configurations: env : - name : INTERFACE value : kfserving - name : PROTOCOL value : v1 env : - name : INTERFACE value : modzy - name : PROTOCOL value : v2","title":"Declares which interfaces it supports"},{"location":"spec/#ports","text":"When started, the container image MUST listen on the TCP port given as the HTTP_PORT environment variable. When a container starts listening on the prescribed port, it MUST adhere to the interface described below corresponding to its configuration.","title":"Ports"},{"location":"spec/#interfaces","text":"The following interfaces are permitted in the spec: kfserving : Supports the KFServing v1 REST API with INTERFACE=kfserving, PROTOCOL=v1 and/or the v2 gRPC API with INTERFACE=kfserving, PROTOCOL=v2 modzy : Supports the Modzy v1 REST API with INTERFACE=modzy, PROTOCOL=v1 or the Modzy v2 gRPC API with INTERFACE=modzy, PROTOCOL=v2 OMI compliant container images MUST implement at least the kfserving.v2 and modzy.v2 APIs. The Chassis reference implementation implements kfserving.v1 , kfserving.v2 and modzy.v2 .","title":"Interfaces"},{"location":"spec/#updates-to-this-spec","text":"If an OMI image declares or supports any strings other than the ones defined above, this document MUST be updated with the new interfaces (with links to their upstream protocol specifications), otherwise the container MUST NOT be described as Open Model Interface compliant. This document can be updated via Pull Request on GitHub . Contributions welcome!","title":"Updates to this spec"}]}